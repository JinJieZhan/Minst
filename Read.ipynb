{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目使用的数据集书MNIST数据集，MNIST数据集一共有7万张图片，其中6万张是训练集，1万张是测试集。\n",
    "项目目标是利用Vision Transformer 处理MINST数据集，其中前期以CNN进行处理达到准确率是98.690000%（Best），后期对Vision Transformer做了如下的变量控制：\n",
    "    （1）数据预处理，归一化操作，将图像值都转换到[-1,1]之间；(input[channel] - mean[channel]) / std[channel]\n",
    "    （2）手写Vision Transformer 与 调用 nn.Module 中的 nn.TransformerEncoder(nn.TransformerEncoderLayer())\n",
    "     (3) 位置编码的使用 Position Embedding\n",
    "    （4）CLS的使用\n",
    "  最终手写的Vision Transformer 的准确率是 0.963800（Best）\n",
    "\n",
    "  项目组织：dataset.py 加载处理数据 vit.py 模型的构建  train.py 训练  infer.py 推断"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、数据预处理，归一化操作，将图像值都转换到[-1,1]之间；(input[channel] - mean[channel]) / std[channel]\n",
    "    处理方式： dataset.py （训练集和数据集一样对应）\n",
    "如果 flag 为 True，只应用 transforms.ToTensor()，即将图像转换为张量。如果 flag 为 False，则应用一个组合的转换，首先将图像转换为张量，然后进行标准化处理 (transforms.Normalize((0.1307,), (0.3081,)))，将图像的像素值标准化到均值为 0.1307 和标准差为 0.3081 的分布。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T07:00:55.925139Z",
     "start_time": "2024-08-10T07:00:55.909515Z"
    }
   },
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "class MNIST_Train(Dataset):\n",
    "    def __init__(self, root: str = '../data/', flag=True, train=True, download=True, batch_size=64):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.download = download\n",
    "        self.batch_size = batch_size\n",
    "        self._transform = transforms.ToTensor() if flag else transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        self.dataset = datasets.MNIST(root=self.root, train=self.train, download=self.download,\n",
    "                                      transform=self._transform)\n",
    "        self.loader = DataLoader(self.dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "    def get_loader(self):\n",
    "        return self.loader\n",
    "\n",
    "\n",
    "class MNIST_Check(Dataset):\n",
    "    def __init__(self, root: str = '../data/', flag=True, batch_size=64):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self._transform = transforms.ToTensor() if flag else transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        self.test_dataset = datasets.MNIST(root=self.root, train=False, download=True, transform=self._transform)\n",
    "        self.test_loader = DataLoader(self.test_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.test_dataset[index]\n",
    "\n",
    "    def get_loader(self):\n",
    "        return self.test_loader\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、 位置编码的使用 Position Embedding\n",
    "    def  __init__\n",
    "        self.pos_emb = nn.Parameter(torch.rand(1, self.patch_count ** 2 + 1, emb_size))  # 如果存在CLS \n",
    "    def forward\n",
    "        x = self.pos_emb + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、 CLS的使用\n",
    "    def  __init__\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_size))  # 分类头输入\n",
    "    def forward\n",
    "        cls_token = self.cls_token.expand(x.size(0), 1, x.size(2))  # (batch_size, 1, emb_size)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # add [cls] token\n",
    "        # x = x.mean(dim=1)  # Example: mean pooling over patches # # 如果没有CLS\n",
    "        return self.cls_linear(y[:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、 手写Vision Transformer 与 调用 nn.Module 中的 nn.TransformerEncoder(nn.TransformerEncoderLayer())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T07:00:55.940768Z",
     "start_time": "2024-08-10T07:00:55.925139Z"
    }
   },
   "source": [
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, emb_size=16, CLSisUsed=True, PostionisUsed=True,nhead=2,num_layer=3):\n",
    "        super().__init__()\n",
    "        self.CLSisUsed = CLSisUsed\n",
    "        self.PostionisUsed = PostionisUsed\n",
    "        self.nhead = nhead\n",
    "        self.num_layer = num_layer\n",
    "        self.patch_size = 4\n",
    "        self.patch_count = 28 // self.patch_size\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=self.patch_size ** 2, kernel_size=self.patch_size, padding=0,\n",
    "                              stride=self.patch_size)\n",
    "        self.patch_emb = nn.Linear(in_features=self.patch_size ** 2, out_features=emb_size)\n",
    "\n",
    "        if self.CLSisUsed:\n",
    "            self.cls_token = nn.Parameter(torch.rand(1, 1, emb_size))\n",
    "        if self.PostionisUsed:\n",
    "            if self.CLSisUsed:\n",
    "                self.pos_emb = nn.Parameter(torch.rand(1, self.patch_count ** 2 + 1, emb_size))\n",
    "            else:\n",
    "                self.pos_emb = nn.Parameter(torch.rand(1, self.patch_count ** 2, emb_size))\n",
    "\n",
    "        self.transformer_enc = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead, batch_first=True), num_layers=num_layer)\n",
    "        self.cls_linear = nn.Linear(in_features=emb_size, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # 输出形状为 (batch_size, 16, height, width)\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # 变形为 (batch_size, 16, 49) 假设 height=7, width=7\n",
    "        x = x.permute(0, 2, 1)  # 变形为 (batch_size, 49, 16)\n",
    "        x = self.patch_emb(x)  # 输出形状为 (batch_size, 49, emb_size\n",
    "\n",
    "        if self.PostionisUsed:\n",
    "            if self.CLSisUsed:\n",
    "                cls_token = self.cls_token.expand(x.size(0), 1, x.size(2))\n",
    "                x = torch.cat((cls_token, x), dim=1)\n",
    "            x = x + self.pos_emb\n",
    "        elif self.CLSisUsed:\n",
    "            cls_token = self.cls_token.expand(x.size(0), 1, x.size(2))\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        y = self.transformer_enc(\n",
    "            x)  # (batch_size, seq_len+1, emb_size) if CLSisUsed, else (batch_size, seq_len, emb_size)\n",
    "\n",
    "        if self.CLSisUsed:\n",
    "            return self.cls_linear(y[:, 0, :])  # (batch_size, num_classes)\n",
    "        else:\n",
    "            x = y.mean(dim=1)  # (batch_size, emb_size)\n",
    "            return self.cls_linear(x)  # (batch_size, num_classes)\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T07:00:55.962321Z",
     "start_time": "2024-08-10T07:00:55.940768Z"
    }
   },
   "source": [
    "# 手写\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# 定义了一个名为 PatchEmbed 的 PyTorch 模型类，用于将图像转换为patch嵌入（Patch Embedding）\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_c, embed_dim, norm_layer=None):\n",
    "        '''\n",
    "        img_size: 输入图像的大小，默认为 28x28 像素。\n",
    "        patch_size: patch的大小，默认为 7x7 像素。\n",
    "        in_c: 输入图像的通道数，默认为 1（适用于灰度图像）。\n",
    "        embed_dim: 嵌入维度，默认为 64，即每个patch被嵌入到的特征向量的维度。\n",
    "        norm_layer: 归一化层的类型，默认为 None。如果提供了，则用它来对嵌入向量进行归一化处理；否则使用恒等映射 nn.Identity()\n",
    "        '''\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)  # 图像的高度和宽度\n",
    "        patch_size = (patch_size, patch_size)  # patch的高度和宽度\n",
    "        # grid_size 计算了图像可以被分成多少个patch。具体来说，它是一个元组，\n",
    "        # 其中第一个元素是图像宽度除以patch宽度的整数部分，第二个元素是图像高度除以patch高度的整数部分。\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])  # 计算了图像被划分为多少个patch\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]  # 代码计算了总的patch数量\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels=in_c, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 定义注意力机制模块\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, qkv_bias, qk_scale, attn_drop_ratio, proj_drop_ratio):\n",
    "        '''\n",
    "        self.num_heads 记录了注意力头的数量。\n",
    "        head_dim 计算每个头的维度。\n",
    "        self.scale 是注意力分数的缩放因子。\n",
    "        self.qkv 是一个线性变换层，将输入向量转换为查询（q）、键（k）、值（v）三部分。\n",
    "        self.attn_drop 和 self.proj_drop 是用于在计算过程中应用的dropout层。\n",
    "        self.proj 是用于将注意力加权后的结果进行投影的线性变换层。\n",
    "        '''\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads  # 将输入向量分为多个头以并行计算注意力\n",
    "        head_dim = dim // num_heads  # 通过整数除法，计算每个头的维度\n",
    "        self.scale = qk_scale or head_dim ** -0.5  # 设置注意力分数的缩放因子\n",
    "        # 输入向量的维度是 dim，输出的维度是 dim * 3，因为每个头需要三个不同的映射（q、k、v）\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        # 用于在计算完注意力后对加权结果进行投影\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        # num_patches 是指图像被划分为的图块数目，或者序列被划分为的片段数目。\n",
    "        # 在某些实现中，为了兼容位置编码（position encoding）或其他需要添加的特殊符号\n",
    "        B, N, C = x.shape\n",
    "        '''\n",
    "        B：批量大小（batch_size）\n",
    "        N：patch 数量加上一个额外的位置编码（num_patches + 1）\n",
    "        C：总嵌入维度（total_embed_dim）\n",
    "        '''\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)  # 对张量的最后一个维度应用 softmax 函数\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features, out_features, drop=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()  # 注意这里调用了 nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)  # 应用 dropout 在激活函数后面\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio,\n",
    "                 qkv_bias,\n",
    "                 qk_scale,\n",
    "                 drop_ratio,\n",
    "                 attn_drop_ratio,\n",
    "                 ):\n",
    "        super(Block, self).__init__()\n",
    "        # self.norm1: 第一个归一化层，使用指定的 norm_layer 对输入进行归一化。\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "\n",
    "        # self.attn: 注意力机制，使用 Attention 类处理输入，包括注意力计算和投影的dropout。\n",
    "        self.attn = Attention(dim=dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "\n",
    "        # self.norm2: 第二个归一化层，再次对输入进行归一化。\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        # self.mlp: 多层感知机模块，使用指定的 Mlp 类进行处理，包括激活函数和dropout。\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=4, in_c=1, num_classes=10,\n",
    "                 embed_dim=16, depth=3, num_heads=2, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=0.5, drop_ratio=0.2, attn_drop_ratio=0.2,\n",
    "                 embed_layer=PatchEmbed, CLSisUsed=False, PostionisUsed=True):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.CLSisUsed = CLSisUsed\n",
    "        self.PostionisUsed = PostionisUsed\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        if self.CLSisUsed:\n",
    "            self.cls_token = nn.Parameter(torch.rand(1, 1, embed_dim))\n",
    "\n",
    "        if self.PostionisUsed:\n",
    "            if self.CLSisUsed:\n",
    "                self.pos_emb = nn.Parameter(torch.rand(1, num_patches + 1, embed_dim))\n",
    "            else:\n",
    "                self.pos_emb = nn.Parameter(torch.rand(1, num_patches, embed_dim))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "        if self.PostionisUsed:\n",
    "            nn.init.trunc_normal_(self.pos_emb, std=0.02)\n",
    "        if self.CLSisUsed:\n",
    "            nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_vit_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        if self.PostionisUsed:\n",
    "            if self.CLSisUsed:\n",
    "                cls_token = self.cls_token.expand(x.size(0), 1, x.size(2))\n",
    "                x = torch.cat((cls_token, x), dim=1)\n",
    "            x = x + self.pos_emb\n",
    "        elif self.CLSisUsed:\n",
    "            cls_token = self.cls_token.expand(x.size(0), 1, x.size(2))\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.forward_features(x)\n",
    "        if self.CLSisUsed:\n",
    "            return self.head(y[:, 0, :])  # (batch_size, num_classes)\n",
    "        else:\n",
    "            x = y.mean(dim=1)  # (batch_size, emb_size)\n",
    "            return self.head(x)  # (batch_size, num_classes)\n",
    "\n",
    "    def _init_vit_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # 卷积层 (nn.Conv2d) 的处理\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # 层归一化 (nn.LayerNorm) 的处理\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T07:01:18.122882Z",
     "start_time": "2024-08-10T07:00:55.962321Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from Look_Best.Vit_Hand import VisionTransformer\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define your dataset\n",
    "dataset = MNIST_Train()\n",
    "\n",
    "# Initialize your ViT model\n",
    "model = VisionTransformer().to(DEVICE)\n",
    "\n",
    "# Attempt to load pretrained model\n",
    "try:\n",
    "    model.load_state_dict(torch.load('Torch_flagTrue_CLSisUsedFalse_PostionisUsedTrue_dim16.pth', map_location=DEVICE))\n",
    "    print(\"Successfully loaded model.pth\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-trained model not found, starting from scratch.\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data loader\n",
    "try:\n",
    "    dataloader = dataset.get_loader()\n",
    "    iter_count = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, (imgs, labels) in enumerate(dataloader):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_count += 1\n",
    "            # Print loss every 1000 iterations\n",
    "            if iter_count % 1000 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{EPOCHS}], Iteration [{iter_count}], Loss: {loss.item()}')\n",
    "        torch.save(model.state_dict(), 'Torch_flagTrue_CLSisUsedFalse_PostionisUsedTrue_dim16.pth')\n",
    "        print(f'Saved model after epoch {epoch + 1}')\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise e\n",
    "\n",
    "dataset = MNIST_Check()\n",
    "dataloader = dataset.get_loader()\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # 适应你的损失函数\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "# 执行评估\n",
    "test_loss, accuracy = evaluate(model, dataloader)\n",
    "\n",
    "# 打印结果\n",
    "print(f'Test Loss: {test_loss:.6f}, Accuracy: {accuracy:.6f}')\n",
    "# Test Loss: 0.310295, Accuracy: 0.901200"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T07:01:55.388228Z",
     "start_time": "2024-08-10T07:01:55.341347Z"
    }
   },
   "source": [
    "# infer.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 在数据准备之前设置设备\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 加载模型到正确的设备\n",
    "model = VisionTransformer().to(DEVICE)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=DEVICE))\n",
    "\n",
    "# 准备数据集和数据加载器\n",
    "dataset = MNIST_Check()\n",
    "dataloader = dataset.get_loader()\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # 适应你的损失函数\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "# 执行评估\n",
    "test_loss, accuracy = evaluate(model, dataloader)\n",
    "\n",
    "# 打印结果\n",
    "print(f'Test Loss: {test_loss:.6f}, Accuracy: {accuracy:.6f}')\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
